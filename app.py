from flask import Flask, render_template, jsonify, request
from dotenv import load_dotenv
from llama_index import SimpleDirectoryReader
from llama_index.node_parser import SimpleNodeParser
from llama_index import ServiceContext, StorageContext
from llama_index import VectorStoreIndex
from llama_index.llms import HuggingFaceInferenceAPI
from llama_index.embeddings import LangchainEmbedding
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from llama_index.vector_stores import CassandraVectorStore
from cassandra.auth import PlainTextAuthProvider
from cassandra.cluster import Cluster
from llama_index import load_index_from_storage
from llama_index import set_global_service_context 
import cassandra
import os


app = Flask(__name__)

load_dotenv()

HF_TOKEN = os.environ.get('HF_TOKEN')

from huggingface_hub import login

login(token=HF_TOKEN) 

#Initializing the Astradb
import json
# This secure connect bundle is autogenerated when you donwload your SCB,
# if yours is different update the file name below
cloud_config= {
  'secure_connect_bundle': 'secure-connect-deep.zip'
}

# This token json file is autogenerated when you donwload your token,
# if yours is different update the file name below
with open("deepanshjha2@gmail.com-token.json") as f:
    secrets = json.load(f)

CLIENT_ID = secrets["clientId"]
CLIENT_SECRET = secrets["secret"]

auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
session = cluster.connect()

row = session.execute("select release_version from system.local").one()
if row:
  print(row[0])
else:
  print("An error occurred.")

PERSIST_DIR = "./storage"
if not os.path.exists(PERSIST_DIR):
    documents = SimpleDirectoryReader("data").load_data()
    parser = SimpleNodeParser()
    nodes = parser.get_nodes_from_documents(documents)
    llm = HuggingFaceInferenceAPI(
    model_name = "HuggingFaceH4/zephyr-7b-alpha",
    api_key = HF_TOKEN
    )
    embed_model = HuggingFaceInferenceAPIEmbeddings(
    api_key=HF_TOKEN, model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,chunk_size=512)
    storage_context = StorageContext.from_defaults() #vector store
    index = VectorStoreIndex(
        nodes,
        service_context=service_context,
        storage_context=storage_context,
    )
    index.storage_context.persist(persist_dir=PERSIST_DIR)
else:
    llm = HuggingFaceInferenceAPI(
    model_name = "HuggingFaceH4/zephyr-7b-alpha",
    api_key = HF_TOKEN
    )
    embed_model = HuggingFaceInferenceAPIEmbeddings(
    api_key=HF_TOKEN, model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,chunk_size=512)
    set_global_service_context(service_context)
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    index = load_index_from_storage(storage_context)


query_engine = index.as_query_engine()

@app.route("/")
def index():
    return render_template('chat.html')



@app.route("/get", methods=["GET", "POST"])
def chat():
    msg = request.form["msg"]
    input = msg
    print("Question : ", input)
    result=query_engine.query(input)
    print("Response : ", result)
    return str(result)



if __name__ == '__main__':
    app.run(host="0.0.0.0", port= 8080, debug= True)


